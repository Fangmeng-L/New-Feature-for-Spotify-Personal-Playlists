{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import spotipy.util as util\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from kneed import KneeLocator\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDR Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotify Developer Credentials and API call to establish an authorised connection for data loading and transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'username':\"<user name>\",'scope':'playlist-read-private',\n",
    "          'client_id':\"<id>\",'client_secret':\"<>\",\n",
    "          'redirect_uri':'http://localhost:8080/callback'}\n",
    "\n",
    "token = util.prompt_for_user_token(config['username'],\n",
    "                                   scope=config['scope'],\n",
    "                                   client_id = config['client_id'],\n",
    "                                   client_secret = config['client_secret'],\n",
    "                                   redirect_uri = config['redirect_uri'])\n",
    "token_write = util.prompt_for_user_token(config['username'],\n",
    "                                   scope= \"playlist-modify-public\",\n",
    "                                   client_id = config['client_id'],\n",
    "                                   client_secret = config['client_secret'],\n",
    "                                   redirect_uri = config['redirect_uri'])\n",
    "\n",
    "\n",
    "sp = spotipy.Spotify(auth=token)\n",
    "sp_write = spotipy.Spotify(auth=token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function below, we make the API call to extract data of a every song in the specified playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(username, playlist_uri,limit=0,songs=[]):\n",
    "    _pid = playlist_uri.split(\":\")[2] # extract the playlist id from the uri\n",
    "    _pname = sp.user_playlist(username,_pid)['name'] # extract name of the playlist by making the API call\n",
    "    \n",
    "    while 1:\n",
    "        results = sp.user_playlist_tracks(username, _pid, offset=limit) # start extracting the details of all the songs in the playlist\n",
    "        songs += results['items']\n",
    "        if results['next'] is not None:\n",
    "            limit = limit + 100 # Max limit to load is 100 songs at a time, so recursively do this until all songs are loaded\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    names = [song['track']['name'] for song in songs] # extracting the metadata (name, artist name and song uri)\n",
    "    artists = [song['track']['artists'][0]['name'] for song in songs]\n",
    "    uris = [song['track']['uri'] for song in songs]\n",
    "    \n",
    "    return _pname, names, artists, uris\n",
    "\n",
    "username = \"<user name>\"\n",
    "playlist_uri = \"<playlist uri>\"\n",
    "\n",
    "playlist, names, artists, uris = api_call(username, playlist_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the API returned data into a dataframe before we write it into Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(names, artists, uris)), columns = ['Name','Artist','URI'])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each song is described by Spotify on 9 parameters, namely: \n",
    "1. Danceability\n",
    "2. Energy\n",
    "3. Loudness\n",
    "4. Speechiness\n",
    "5. Acousticness\n",
    "6. Instrumentalness\n",
    "7. Liveness\n",
    "8. Valence\n",
    "9. Tempo\n",
    "\n",
    "So, we try and extract these features of each and every song in our playlist. Later, we use these features to cluster these songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dict = sp.audio_features(uris[0])[0] # reference dictionary to keep track of all the available variables for each song\n",
    "\n",
    "def feature_extraction(row):\n",
    "    for key,_ in reference_dict.items():\n",
    "        row[key] = sp.audio_features(row['URI'])[0][key]\n",
    "    return row\n",
    "\n",
    "df = df.progress_apply(feature_extraction, axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save these files into Mongo DB (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "url = \"mongodb://localhost:27017\"\n",
    "db = \"spotify\"\n",
    "collection = \"my_playlist\"\n",
    "client = MongoClient(host=url)\n",
    "database = client[db]\n",
    "collection = database[collection]\n",
    "\n",
    "collection.insert_many(df.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few irrelevant features in our data like the URI, type, id, track_href and analysis url which do not play any role in clustering the similar songs. So we drop those features as a part of our data preprocessing and push the updated dataframe into a new collection on Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = database['processed_data']\n",
    "\n",
    "df.drop(['type','id','uri','track_href','analysis_url','key','mode'],axis=1,inplace=True)\n",
    "\n",
    "collection.insert_many(df.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing on the rank/serial number in the mongo\n",
    "\n",
    "collection.create_index(\"duration_ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Next we perform PCA to compress the available 9 features set into a fewer dimensional space. Before we perform PCA, we scale the data to standardize it. If this step is avoided, the clustering algorithm might assume extra weight to features like loudness and tempo as they vary by much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.iloc[:,3:-2]) # # excluding the string variables and other irrelevant columns from scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "# The attribute shows how much variance is explained by each of the nine features\n",
    "variance_explain = pca.explained_variance_ratio_\n",
    "variance_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, len(df.iloc[:,3:-2].columns)+1), variance_explain.cumsum(), marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components', fontsize=18)\n",
    "plt.ylabel('Cumulative Explained Variance',fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now try to find the optimal number of principal components to consider which would explain at least 80% of variation\n",
    "\n",
    "for i, exp_var in enumerate(variance_explain.cumsum()):\n",
    "    if exp_var >= 0.8:\n",
    "        n_vectors = i + 1\n",
    "        break\n",
    "print(\"Number of components:\", n_vectors)\n",
    "pca = PCA(n_components=n_vectors)\n",
    "pca.fit(X)\n",
    "scores_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame({'danceability':-1*pca.components_[:,0],'energy':-1*pca.components_[:,1],\n",
    "             'loudness':-1*pca.components_[:,2],'specchiness':-1*pca.components_[:,3],\n",
    "             'acousticness':-1*pca.components_[:,4],'instrumentalness':-1*pca.components_[:,5],\n",
    "             'liveness':-1*pca.components_[:,6],'valence':-1*pca.components_[:,7],'tempo':-1*pca.components_[:,8]}).T\n",
    "pca_df[pca_df < 0.15] = None\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we narrow down on 6 principal components and the PCs can be renamed as follows:      \n",
    "      \n",
    "Principal Component 1: Contains Energy, Loudness (abb: EL)      \n",
    "Principal Component 2: Contains Danceability, Valence (abb: DV)        \n",
    "Principal Component 3: Contains Dancebility and Instrumentalness (abb: DI)       \n",
    "Principal Component 4: Contains Instrumentalness and Tempo (abb: IT)          \n",
    "Principal Component 5: Contains Acousticness (abb: A)          \n",
    "Principal Component 6: Contains Danceability and Tempo (abb: DT)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(KMeans(init='k-means++', random_state=42), k=(1,21), timings=False)\n",
    "visualizer.fit(scores_pca)\n",
    "visualizer.show()\n",
    "n_clusters = visualizer.elbow_value_\n",
    "print(\"Optimal number of clusters:\", n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_ = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "kmeans_.fit(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = df.iloc[:,2:-2]\n",
    "df_kmpca = pd.concat([features_df.reset_index(drop=True), pd.DataFrame(scores_pca)], axis=1)\n",
    "df_kmpca.columns.values[(-1*n_vectors):] = [\"Component \" + str(i+1) for i in range(n_vectors)]\n",
    "df_kmpca['Cluster'] = kmeans_.labels_\n",
    "df_kmpca.rename(columns= {'Component 1':'EL','Component 2':'DV','Component 3':'DI',\n",
    "                                  'Component 4':'IT','Component 5':'A','Component 6':'DT'},inplace=True)\n",
    "df_kmpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = df_kmpca['Cluster']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Component 1':'EL','Component 2':'DV','Component 3':'DI',\n",
    "#                                 'Component 4':'IT','Component 5':'A','Component 6':'DT'\n",
    "\n",
    "df['Cluster'] = df_kmpca['Cluster']\n",
    "df['EL'] = df_kmpca['EL']\n",
    "df['DV']= df_kmpca['DV']\n",
    "x = df['EL']\n",
    "y = df['DV']\n",
    "n = df['Cluster']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x, y, hue=df['Cluster'], palette = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'goldenrod', 'tab:cyan'])\n",
    "plt.title('Clusters by PCA Components', fontsize=20)\n",
    "plt.xlabel(\"Energy/Loudness\", fontsize=18)\n",
    "plt.ylabel(\"Danceability/Valence\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "for i, txt in enumerate(n):\n",
    "    plt.text(x[i], y[i],txt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Component 1':'EL','Component 2':'DV','Component 3':'DI',\n",
    "#                                 'Component 4':'IT','Component 5':'A','Component 6':'DT'\n",
    "\n",
    "df['Cluster'] = df_kmpca['Cluster']\n",
    "df['A'] = df_kmpca['A']\n",
    "df['DT']= df_kmpca['DT']\n",
    "x = df['A']\n",
    "y = df['DT']\n",
    "n = df['Cluster']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x, y, hue=df['Cluster'], palette = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'goldenrod', 'tab:cyan'])\n",
    "plt.title('Clusters by PCA Components', fontsize=20)\n",
    "plt.xlabel(\"Acousticness\", fontsize=18)\n",
    "plt.ylabel(\"Danceability/Tempo\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "for i, txt in enumerate(n):\n",
    "    plt.text(x[i], y[i],txt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Component 1':'EL','Component 2':'DV','Component 3':'DI',\n",
    "#                                 'Component 4':'IT','Component 5':'A','Component 6':'DT'\n",
    "\n",
    "df['Cluster'] = df_kmpca['Cluster']\n",
    "df['A'] = df_kmpca['A']\n",
    "df['IT']= df_kmpca['IT']\n",
    "x = df['A']\n",
    "y = df['IT']\n",
    "n = df['Cluster']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x, y, hue=df['Cluster'], palette = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'goldenrod', 'tab:cyan'])\n",
    "plt.title('Clusters by PCA Components', fontsize=20)\n",
    "plt.xlabel(\"Acousticness\", fontsize=18)\n",
    "plt.ylabel(\"Instrumental/Tempo\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "for i, txt in enumerate(n):\n",
    "    plt.text(x[i], y[i],txt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cluster 0 - Low Instrumental / Low Acousticness / High Danceability / High Valence = PARTY\n",
    "Cluster 1 - Low Energy/ Low Loudness/ Low Danceability/ Low Valence = SAD\n",
    "Cluster 2 - High Instrumental / High Tempo / High Acousticness / Low Energy/ Low Loudness = STUDY\n",
    "Cluster 3 - Low Acousticness / High Energy/ High Loudness/ High Danceability/ High Valence = GYM\n",
    "Cluster 4 - Low Danceability = TRAVEL\n",
    "Cluster 5 - High Acousticness / Low Energy/ Low Loudness/ High Danceability/ High Valence = HAPPY\n",
    "Cluster 6 - High Energy/ High Loudness/ Low Danceability/ Low Valence = OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {0:\"Cluster 0 - Party\", 1:\"Cluster 1 - SAD\", 2:\"Cluster 2 - STUDY\", 3:\"Cluster 3 - GYM\", 4:\"Cluster 4 - TRAVEL\", \n",
    "            5:\"Cluster 5 - HAPPY\", 6:\"Cluster 6 - OTHER\"}\n",
    "\n",
    "for key,value in clusters.items():\n",
    "    temp = sp_write.user_playlist_create(config['username'], value, public=True, collaborative=False, description='')\n",
    "    _pid = temp['id']\n",
    "    songs = list(df.loc[df['Cluster'] == key]['URI'])\n",
    "    if len(songs) > 100:\n",
    "        sp_write.playlist_add_items(_pid, songs[:100])\n",
    "        sp_write.playlist_add_items(_pid, songs[100:])\n",
    "    else:\n",
    "        sp_write.playlist_add_items(_pid, songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
